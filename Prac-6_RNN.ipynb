{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name : Tirth Hihoriya\n",
    "## Roll no : 18bce244\n",
    "## Prac- 6 : RNN for Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:  I often get nostalgic remembering the cherished moments which I shared with Shanky. \n",
      "We used to play game after game of chess, for hours at a stretch. \n",
      "We shared, all our notes and often helped each other with school assignments and projects. \n",
      "He was party to all my pranks which we planned at our secret hideout, the tree-house. <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "data = \"\"\" I often get nostalgic remembering the cherished moments which I shared with Shanky. \n",
    "We used to play game after game of chess, for hours at a stretch. \n",
    "We shared, all our notes and often helped each other with school assignments and projects. \n",
    "He was party to all my pranks which we planned at our secret hideout, the tree-house.\"\"\"\n",
    "print (\"Data:\", data, type(data) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data_Splitted: [' I often get nostalgic remembering the cherished moments which I shared with Shanky. ', 'We used to play game after game of chess, for hours at a stretch. ', 'We shared, all our notes and often helped each other with school assignments and projects. ', 'He was party to all my pranks which we planned at our secret hideout, the tree-house.'] <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "data_splitted=data.split('\\n') #returns a list of strings\n",
    "print(\"Data_Splitted:\", data_splitted, type(data_splitted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.text.Tokenizer at 0x1944cb7f4c0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer=Tokenizer(filters='!\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n',)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Indices: {'we': 1, 'i': 2, 'often': 3, 'the': 4, 'which': 5, 'shared': 6, 'with': 7, 'to': 8, 'game': 9, 'at': 10, 'all': 11, 'our': 12, 'and': 13, 'get': 14, 'nostalgic': 15, 'remembering': 16, 'cherished': 17, 'moments': 18, 'shanky.': 19, 'used': 20, 'play': 21, 'after': 22, 'of': 23, 'chess': 24, 'for': 25, 'hours': 26, 'a': 27, 'stretch.': 28, 'notes': 29, 'helped': 30, 'each': 31, 'other': 32, 'school': 33, 'assignments': 34, 'projects.': 35, 'he': 36, 'was': 37, 'party': 38, 'my': 39, 'pranks': 40, 'planned': 41, 'secret': 42, 'hideout': 43, 'tree': 44, 'house.': 45}\n"
     ]
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(data_splitted)\n",
    "print(\"Word Indices:\", tokenizer.word_index) #tokenizer.word_index is a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 46\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer .word_index) + 1\n",
    "print (\"Vocab Size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences: [[2, 3, 14, 15, 16, 4, 17, 18, 5, 2, 6, 7, 19], [1, 20, 8, 21, 9, 22, 9, 23, 24, 25, 26, 10, 27, 28], [1, 6, 11, 12, 29, 13, 3, 30, 31, 32, 7, 33, 34, 13, 35], [36, 37, 38, 8, 11, 39, 40, 5, 1, 41, 10, 12, 42, 43, 4, 44, 45]] \n",
      " <class 'list'> 4\n"
     ]
    }
   ],
   "source": [
    "sequences=tokenizer.texts_to_sequences(data_splitted)  #list of 4 list\n",
    "l=len(sequences)\n",
    "print(\"Sequences:\", sequences,\"\\n\", type(sequences), l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X= [[2, 3, 14, 15, 16, 4, 17, 18, 5, 2, 6, 7], [1, 20, 8, 21, 9, 22, 9, 23, 24, 25, 26, 10, 27], [1, 6, 11, 12, 29, 13, 3, 30, 31, 32, 7, 33, 34, 13], [36, 37, 38, 8, 11, 39, 40, 5, 1, 41, 10, 12, 42, 43, 4, 44]] <class 'list'> \n",
      "y= [[2, 3, 14, 15, 16, 4, 17, 18, 5, 2, 6, 7, 19], [1, 20, 8, 21, 9, 22, 9, 23, 24, 25, 26, 10, 27, 28], [1, 6, 11, 12, 29, 13, 3, 30, 31, 32, 7, 33, 34, 13, 35], [36, 37, 38, 8, 11, 39, 40, 5, 1, 41, 10, 12, 42, 43, 4, 44, 45]] <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "X = list()\n",
    "y = list()\n",
    "\n",
    "for i in range(l):\n",
    "    X.insert(i,sequences[i][:-1])\n",
    "    y.insert(i,sequences[i])\n",
    "\n",
    "print(\"X=\", X , type(X), \"\\ny=\", y, type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen = max([len(sequence) for sequence in X])\n",
    "maxlen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X= [[ 0  0  0  0  0  2  3 14 15 16  4 17 18  5  2  6  7]\n",
      " [ 0  0  0  0  1 20  8 21  9 22  9 23 24 25 26 10 27]\n",
      " [ 0  0  0  1  6 11 12 29 13  3 30 31 32  7 33 34 13]\n",
      " [ 0 36 37 38  8 11 39 40  5  1 41 10 12 42 43  4 44]] <class 'numpy.ndarray'> Shape =  (4, 17)\n",
      "\n",
      "y= [[ 0  0  0  0  2  3 14 15 16  4 17 18  5  2  6  7 19]\n",
      " [ 0  0  0  1 20  8 21  9 22  9 23 24 25 26 10 27 28]\n",
      " [ 0  0  1  6 11 12 29 13  3 30 31 32  7 33 34 13 35]\n",
      " [36 37 38  8 11 39 40  5  1 41 10 12 42 43  4 44 45]] <class 'numpy.ndarray'> Shape =  (4, 17)\n"
     ]
    }
   ],
   "source": [
    "X = pad_sequences(X, maxlen=maxlen+1, padding='pre')\n",
    "print(\"X=\", X , type(X),\"Shape = \", X.shape)\n",
    "\n",
    "y = pad_sequences(y, maxlen=maxlen+1, padding='pre')\n",
    "print(\"\\ny=\", y , type(y),\"Shape = \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "y= [[[1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 1. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]] <class 'numpy.ndarray'> Shape =  (4, 17, 46)\n"
     ]
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes = vocab_size)\n",
    "print(\"\\ny=\", y , type(y),\"Shape = \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 10)          460       \n",
      "_________________________________________________________________\n",
      "simple_rnn_2 (SimpleRNN)     (None, None, 50)          3050      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, None, 46)          2346      \n",
      "=================================================================\n",
      "Total params: 5,856\n",
      "Trainable params: 5,856\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = vocab_size, output_dim=10))\n",
    "model.add(SimpleRNN(units = 50, return_sequences = True))\n",
    "model.add(Dense(units=vocab_size, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam',\n",
    "              loss= 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2638 - accuracy: 0.9559\n",
      "Epoch 2/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2611 - accuracy: 0.9559\n",
      "Epoch 3/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2585 - accuracy: 0.9559\n",
      "Epoch 4/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2559 - accuracy: 0.9559\n",
      "Epoch 5/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2533 - accuracy: 0.9559\n",
      "Epoch 6/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2509 - accuracy: 0.9559\n",
      "Epoch 7/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2484 - accuracy: 0.9559\n",
      "Epoch 8/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2461 - accuracy: 0.9559\n",
      "Epoch 9/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2437 - accuracy: 0.9559\n",
      "Epoch 10/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2415 - accuracy: 0.9559\n",
      "Epoch 11/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2392 - accuracy: 0.9559\n",
      "Epoch 12/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2370 - accuracy: 0.9559\n",
      "Epoch 13/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2349 - accuracy: 0.9559\n",
      "Epoch 14/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2328 - accuracy: 0.9559\n",
      "Epoch 15/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2307 - accuracy: 0.9559\n",
      "Epoch 16/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2287 - accuracy: 0.9559\n",
      "Epoch 17/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2267 - accuracy: 0.9559\n",
      "Epoch 18/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2247 - accuracy: 0.9559\n",
      "Epoch 19/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2228 - accuracy: 0.9559\n",
      "Epoch 20/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2210 - accuracy: 0.9559\n",
      "Epoch 21/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2191 - accuracy: 0.9559\n",
      "Epoch 22/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2173 - accuracy: 0.9559\n",
      "Epoch 23/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2155 - accuracy: 0.9559\n",
      "Epoch 24/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2138 - accuracy: 0.9559\n",
      "Epoch 25/180\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2121 - accuracy: 0.9559\n",
      "Epoch 26/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2104 - accuracy: 0.9559\n",
      "Epoch 27/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2088 - accuracy: 0.9559\n",
      "Epoch 28/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2072 - accuracy: 0.9559\n",
      "Epoch 29/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2056 - accuracy: 0.9559\n",
      "Epoch 30/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2041 - accuracy: 0.9559\n",
      "Epoch 31/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2025 - accuracy: 0.9559\n",
      "Epoch 32/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2010 - accuracy: 0.9559\n",
      "Epoch 33/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1996 - accuracy: 0.9559\n",
      "Epoch 34/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1981 - accuracy: 0.9559\n",
      "Epoch 35/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1967 - accuracy: 0.9559\n",
      "Epoch 36/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1953 - accuracy: 0.9559\n",
      "Epoch 37/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1940 - accuracy: 0.9559\n",
      "Epoch 38/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1926 - accuracy: 0.9559\n",
      "Epoch 39/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1913 - accuracy: 0.9559\n",
      "Epoch 40/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1900 - accuracy: 0.9559\n",
      "Epoch 41/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1888 - accuracy: 0.9559\n",
      "Epoch 42/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1875 - accuracy: 0.9559\n",
      "Epoch 43/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1863 - accuracy: 0.9559\n",
      "Epoch 44/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1851 - accuracy: 0.9559\n",
      "Epoch 45/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1839 - accuracy: 0.9559\n",
      "Epoch 46/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1827 - accuracy: 0.9559\n",
      "Epoch 47/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1816 - accuracy: 0.9559\n",
      "Epoch 48/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1805 - accuracy: 0.9559\n",
      "Epoch 49/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1794 - accuracy: 0.9559\n",
      "Epoch 50/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1783 - accuracy: 0.9559\n",
      "Epoch 51/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1772 - accuracy: 0.9559\n",
      "Epoch 52/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1762 - accuracy: 0.9559\n",
      "Epoch 53/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1751 - accuracy: 0.9559\n",
      "Epoch 54/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1741 - accuracy: 0.9559\n",
      "Epoch 55/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1731 - accuracy: 0.9559\n",
      "Epoch 56/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1721 - accuracy: 0.9559\n",
      "Epoch 57/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1712 - accuracy: 0.9559\n",
      "Epoch 58/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1702 - accuracy: 0.9559\n",
      "Epoch 59/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1693 - accuracy: 0.9559\n",
      "Epoch 60/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1684 - accuracy: 0.9559\n",
      "Epoch 61/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1675 - accuracy: 0.9559\n",
      "Epoch 62/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1666 - accuracy: 0.9559\n",
      "Epoch 63/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1657 - accuracy: 0.9559\n",
      "Epoch 64/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1648 - accuracy: 0.9559\n",
      "Epoch 65/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1640 - accuracy: 0.9559\n",
      "Epoch 66/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1632 - accuracy: 0.9559\n",
      "Epoch 67/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1623 - accuracy: 0.9559\n",
      "Epoch 68/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1615 - accuracy: 0.9559\n",
      "Epoch 69/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1607 - accuracy: 0.9559\n",
      "Epoch 70/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1600 - accuracy: 0.9559\n",
      "Epoch 71/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1592 - accuracy: 0.9559\n",
      "Epoch 72/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1584 - accuracy: 0.9559\n",
      "Epoch 73/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1577 - accuracy: 0.9559\n",
      "Epoch 74/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1569 - accuracy: 0.9559\n",
      "Epoch 75/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1562 - accuracy: 0.9559\n",
      "Epoch 76/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1555 - accuracy: 0.9559\n",
      "Epoch 77/180\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1548 - accuracy: 0.9559\n",
      "Epoch 78/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1541 - accuracy: 0.9559\n",
      "Epoch 79/180\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1534 - accuracy: 0.9559\n",
      "Epoch 80/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1527 - accuracy: 0.9559\n",
      "Epoch 81/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1521 - accuracy: 0.9559\n",
      "Epoch 82/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1514 - accuracy: 0.9559\n",
      "Epoch 83/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1508 - accuracy: 0.9559\n",
      "Epoch 84/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1502 - accuracy: 0.9559\n",
      "Epoch 85/180\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1495 - accuracy: 0.9559\n",
      "Epoch 86/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1489 - accuracy: 0.9559\n",
      "Epoch 87/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1483 - accuracy: 0.9559\n",
      "Epoch 88/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1477 - accuracy: 0.9559\n",
      "Epoch 89/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1471 - accuracy: 0.9559\n",
      "Epoch 90/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1465 - accuracy: 0.9559\n",
      "Epoch 91/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1460 - accuracy: 0.9559\n",
      "Epoch 92/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1454 - accuracy: 0.9559\n",
      "Epoch 93/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1448 - accuracy: 0.9559\n",
      "Epoch 94/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1443 - accuracy: 0.9559\n",
      "Epoch 95/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1438 - accuracy: 0.9559\n",
      "Epoch 96/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1432 - accuracy: 0.9559\n",
      "Epoch 97/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1427 - accuracy: 0.9559\n",
      "Epoch 98/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1422 - accuracy: 0.9559\n",
      "Epoch 99/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1417 - accuracy: 0.9559\n",
      "Epoch 100/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1412 - accuracy: 0.9559\n",
      "Epoch 101/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1407 - accuracy: 0.9559\n",
      "Epoch 102/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1402 - accuracy: 0.9559\n",
      "Epoch 103/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1397 - accuracy: 0.9559\n",
      "Epoch 104/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1392 - accuracy: 0.9559\n",
      "Epoch 105/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1387 - accuracy: 0.9559\n",
      "Epoch 106/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1383 - accuracy: 0.9559\n",
      "Epoch 107/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1378 - accuracy: 0.9559\n",
      "Epoch 108/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1373 - accuracy: 0.9559\n",
      "Epoch 109/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1369 - accuracy: 0.9559\n",
      "Epoch 110/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1365 - accuracy: 0.9559\n",
      "Epoch 111/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1360 - accuracy: 0.9559\n",
      "Epoch 112/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1356 - accuracy: 0.9559\n",
      "Epoch 113/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1352 - accuracy: 0.9559\n",
      "Epoch 114/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1347 - accuracy: 0.9559\n",
      "Epoch 115/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1343 - accuracy: 0.9559\n",
      "Epoch 116/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1339 - accuracy: 0.9559\n",
      "Epoch 117/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1335 - accuracy: 0.9559\n",
      "Epoch 118/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1331 - accuracy: 0.9559\n",
      "Epoch 119/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1327 - accuracy: 0.9559\n",
      "Epoch 120/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1323 - accuracy: 0.9559\n",
      "Epoch 121/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1319 - accuracy: 0.9559\n",
      "Epoch 122/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1316 - accuracy: 0.9559\n",
      "Epoch 123/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1312 - accuracy: 0.9559\n",
      "Epoch 124/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1308 - accuracy: 0.9559\n",
      "Epoch 125/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1305 - accuracy: 0.9559\n",
      "Epoch 126/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1301 - accuracy: 0.9559\n",
      "Epoch 127/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1297 - accuracy: 0.9559\n",
      "Epoch 128/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1294 - accuracy: 0.9559\n",
      "Epoch 129/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1290 - accuracy: 0.9559\n",
      "Epoch 130/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1287 - accuracy: 0.9559\n",
      "Epoch 131/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1283 - accuracy: 0.9559\n",
      "Epoch 132/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1280 - accuracy: 0.9559\n",
      "Epoch 133/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1277 - accuracy: 0.9559\n",
      "Epoch 134/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1273 - accuracy: 0.9559\n",
      "Epoch 135/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1270 - accuracy: 0.9559\n",
      "Epoch 136/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1267 - accuracy: 0.9559\n",
      "Epoch 137/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1264 - accuracy: 0.9559\n",
      "Epoch 138/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1261 - accuracy: 0.9559\n",
      "Epoch 139/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1258 - accuracy: 0.9559\n",
      "Epoch 140/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1254 - accuracy: 0.9559\n",
      "Epoch 141/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1251 - accuracy: 0.9559\n",
      "Epoch 142/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1248 - accuracy: 0.9559\n",
      "Epoch 143/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1245 - accuracy: 0.9559\n",
      "Epoch 144/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1243 - accuracy: 0.9559\n",
      "Epoch 145/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1240 - accuracy: 0.9559\n",
      "Epoch 146/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1237 - accuracy: 0.9559\n",
      "Epoch 147/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1234 - accuracy: 0.9559\n",
      "Epoch 148/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1231 - accuracy: 0.9559\n",
      "Epoch 149/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1228 - accuracy: 0.9559\n",
      "Epoch 150/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1226 - accuracy: 0.9559\n",
      "Epoch 151/180\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1223 - accuracy: 0.9559\n",
      "Epoch 152/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1220 - accuracy: 0.9559\n",
      "Epoch 153/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1218 - accuracy: 0.9559\n",
      "Epoch 154/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1215 - accuracy: 0.9559\n",
      "Epoch 155/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1212 - accuracy: 0.9559\n",
      "Epoch 156/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1210 - accuracy: 0.9559\n",
      "Epoch 157/180\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1207 - accuracy: 0.9559\n",
      "Epoch 158/180\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1205 - accuracy: 0.9559\n",
      "Epoch 159/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1202 - accuracy: 0.9559\n",
      "Epoch 160/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1200 - accuracy: 0.9559\n",
      "Epoch 161/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1197 - accuracy: 0.9559\n",
      "Epoch 162/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1195 - accuracy: 0.9559\n",
      "Epoch 163/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1193 - accuracy: 0.9559\n",
      "Epoch 164/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1190 - accuracy: 0.9559\n",
      "Epoch 165/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1188 - accuracy: 0.9559\n",
      "Epoch 166/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1186 - accuracy: 0.9559\n",
      "Epoch 167/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1183 - accuracy: 0.9559\n",
      "Epoch 168/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1181 - accuracy: 0.9559\n",
      "Epoch 169/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1179 - accuracy: 0.9559\n",
      "Epoch 170/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1176 - accuracy: 0.9559\n",
      "Epoch 171/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1174 - accuracy: 0.9559\n",
      "Epoch 172/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1172 - accuracy: 0.9559\n",
      "Epoch 173/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1170 - accuracy: 0.9559\n",
      "Epoch 174/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1168 - accuracy: 0.9559\n",
      "Epoch 175/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1166 - accuracy: 0.9559\n",
      "Epoch 176/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1164 - accuracy: 0.9559\n",
      "Epoch 177/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1162 - accuracy: 0.9559\n",
      "Epoch 178/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1159 - accuracy: 0.9559\n",
      "Epoch 179/180\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1157 - accuracy: 0.9559\n",
      "Epoch 180/180\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1155 - accuracy: 0.9559\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1944e5ae0d0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Input Sentence:  game of chess after school\n",
      "Encoded :  [[ 0  9 23 24 22 33]] (1, 6) 2\n",
      "Prob:  (1, 6, 46)\n",
      "\n",
      "Prob of sentence -  \" game of chess after school \" is :  2.604424797777865e-18\n"
     ]
    }
   ],
   "source": [
    "# generate probability of an input sentence. This estimate is P(w1)\"P(w2[w1)\"P(w3|w1, w2)\"P(w4|w1, w2, w3)\".....\n",
    "# This is because model was trained to predict the next word based on the all preceding word\n",
    "\n",
    "def prob_of_input_sentence(model, tokenizer, input_sentence):\n",
    "    print(\"\\n\\nInput Sentence: \", input_sentence)\n",
    "    encoded = tokenizer.texts_to_sequences([input_sentence])[0]\n",
    "#     print(\"Encoded  :\", encoded)\n",
    "    encoded.insert(0,0)\n",
    "\n",
    "    encoded=array(encoded)\n",
    "    encoded=np.reshape( encoded, newshape=(1, -1))\n",
    "    print(\"Encoded : \", encoded, encoded.shape, encoded.ndim)\n",
    "    \n",
    "    prob = model.predict_proba(encoded, verbose=0)\n",
    "    print(\"Prob: \", prob.shape)\n",
    "    \n",
    "    probability = 1\n",
    "    for i in range(prob.shape[1]-1):\n",
    "        probability = probability*prob[0,i,encoded[0,i+1]]\n",
    "    print(\"\\nProb of sentence - \", \"\\\"\", input_sentence , \"\\\"\", \"is : \", probability)\n",
    "    \n",
    "    \n",
    "prob_of_input_sentence(model, tokenizer, \"game of chess after school\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Input Sentence:   other with school assignments and projects\n",
      "Encoded [[ 0 32  7 33 34 13]] (1, 6)\n",
      "Prob of sentence -  \"  other with school assignments and projects \" is :  3.018147887654018e-20\n"
     ]
    }
   ],
   "source": [
    "prob_of_input_sentence(model, tokenizer, \" other with school assignments and projects\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Input Sentence:  all our notes\n",
      "Encoded [[ 0 11 12 29]] (1, 4)\n",
      "Prob of sentence -  \" all our notes \" is :  3.5313143081888116e-10\n"
     ]
    }
   ],
   "source": [
    "prob_of_input_sentence (model, tokenizer, \"all our notes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Input Sentence:  to fetch a pail of water\n",
      "Encoded [[ 0  8 27 23]] (1, 4)\n",
      "Prob of sentence -  \" to fetch a pail of water \" is :  8.069660599102207e-12\n"
     ]
    }
   ],
   "source": [
    "prob_of_input_sentence(model, tokenizer, \"to fetch a pail of water\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
